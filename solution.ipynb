{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes Classification\n",
    "\n",
    "## About dataset\n",
    "- The Behavioral Risk Factor Surveillance System (BRFSS) is a health-related telephone survey that collects data from U.S. residents on their health-related risk behaviors, chronic health conditions, and use of preventive services\n",
    "- The dataset has been established in 1984 with 15 states, it now collects data from all 50 states, D.C., and 3 U.S. territories\n",
    "- Over 400,000 adult interviews are completed each year, making it the largest continuous health survey system in the world\n",
    "- Factors assessed include tobacco use, healthcare coverage, HIV/AIDS knowledge/prevention, physical activity, and fruit/vegetable consumption\n",
    "- A record in the data corresponds to a single respondent (each from a single household)\n",
    "- The description of columns can be found in the linked PDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features description\n",
    "| Feature               | Description                                                                  |\n",
    "|-----------------------|------------------------------------------------------------------------------|\n",
    "| diabetes              | Subject was told they have diabetes                                          |\n",
    "| high_blood_pressure   | Subject has high blood pressure                                              |\n",
    "| high_cholesterol      | Subject has high cholesterol                                                 |\n",
    "| cholesterol_check     | Subject had cholesterol check within the last five years                     |\n",
    "| bmi                   | BMI of the subject                                                           |\n",
    "| smoked_100_cigarettes | Subject has smoked at least 100 cigarettes during their life                 |\n",
    "| stroke                | Subject experienced stroke during their life                                 |\n",
    "| coronary_disease      | Subject has/had coronary heart disease or myocardial infarction              |\n",
    "| exercise              | Subject does regular exercise or physical activity                           |\n",
    "| consumes_fruit        | Subject consumes fruits at least once a day                                  |\n",
    "| consumes_vegetables   | Subject consumes vegetables at least once a day                              |\n",
    "| heavy_alcohol_drinker | Heavy drinkers are defined as adult men having more than 14 drinks per week |\n",
    "| insurance             | Subject has some kind of health plan (insurance, prepaid plans, ...)         |\n",
    "| no_doctor_money       | Subject was unable to visit doctor in the past 12 months because of cost     |\n",
    "| health                | How good is the health of the subject (self rated)                           |\n",
    "| mental_health         | Number of days in the past month when subject's mental health was not good   |\n",
    "| physical_health       | Number of days in the past month when subject's physical health was not good |\n",
    "| climb_difficulty      | Subject has difficulties climbing stairs                                     |\n",
    "| sex                   | Sex of the subject                                                           |\n",
    "| age_category          | Age category of the subject                                                  |\n",
    "| educatation_level     | Highest level of education achieved by the subject                           |\n",
    "| income                | Income of subject's household                                                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset. All 5 parts are concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from core import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do basic preprocessing on columns and categorical values in order to make the dataset more humanly readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import process_columns, remove_unusable_diabetes_categories\n",
    "\n",
    "process_columns(dataset)\n",
    "\n",
    "# 'Unnamed: 0' is a duplicate column of ID\n",
    "dataset.drop(\"Unnamed: 0\", axis=\"columns\", inplace=True)\n",
    "\n",
    "dataset.drop_duplicates(inplace=True)\n",
    "\n",
    "    # ID is no longer needed after dropping duplicates\n",
    "dataset.drop(\"ID\", axis=\"columns\", inplace=True)\n",
    "\n",
    "# Remove rows where target label is missing\n",
    "dataset = dataset[~dataset[\"diabetes\"].isna()]\n",
    "\n",
    "# Remove pre diabetes and diabetes in pregnancy categories\n",
    "dataset_without_extended_diabetes_categories = remove_unusable_diabetes_categories(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a baseline model to have something to compare our more advanced models to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = dataset_without_extended_diabetes_categories\n",
    "X, y = dataset.drop(\"diabetes\", axis=\"columns\"), dataset[\"diabetes\"]\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=0)\n",
    "\n",
    "baseline = DummyClassifier(strategy=\"uniform\", random_state=0)\n",
    "baseline.fit(train_X, train_y)\n",
    "print(classification_report(test_y, baseline.predict(test_X), zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "Perceptron()\n",
    "# Assuming dataset_with_extended_diabetes_categories is a pandas DataFrame\n",
    "dataset = dataset_without_extended_diabetes_categories\n",
    "\n",
    "# Splitting dataset into features and target\n",
    "X = dataset.drop(\"diabetes\", axis=1)\n",
    "y = dataset[\"diabetes\"]\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Splitting dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create preprocessing steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OrdinalEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\"))]\n",
    ")\n",
    "\n",
    "# Create the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_transformer, make_column_selector(dtype_include=\"category\")),\n",
    "        (\"num\", numerical_transformer, make_column_selector(dtype_exclude=\"category\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "random_forest_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            RandomForestClassifier(\n",
    "                random_state=42,\n",
    "                class_weight=\"balanced_subsample\",\n",
    "                max_depth=12,\n",
    "                max_features=\"sqrt\",\n",
    "                max_leaf_nodes=50,\n",
    "                min_samples_leaf=50,\n",
    "                n_estimators=120,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# \n",
    "# param_grid = {\n",
    "#     \"classifier__class_weight\": [\"balanced\", \"balanced_subsample\", None],\n",
    "#     \"classifier__max_depth\": [12, 15, 20, None],\n",
    "#     \"classifier__max_features\": [None, \"sqrt\", \"log2\"],\n",
    "#     \"classifier__min_samples_leaf\": [3, 3**2, 3**3, 3**4],\n",
    "#     \"classifier__max_leaf_nodes\": [25, 50, 100, None],\n",
    "#     \"classifier__min_samples_leaf\": [3, 3**2, 3**3, 3**4],\n",
    "#     \"classifier__n_estimators\": [40, 80, 120, 160, 200],\n",
    "# }\n",
    "# \n",
    "# clf = GridSearchCV(random_forest_model, param_grid, scoring=\"recall_macro\", n_jobs=-1)\n",
    "# clf.fit(X_train[:3000], y_train[:3000])\n",
    "# \n",
    "# print(clf.best_params_)\n",
    "# print(classification_report(y_test, clf.predict(X_test)))\n",
    "\n",
    "random_forest_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our first model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def encode_diabetes_column(diabetes_column):\n",
    "    \"\"\"\n",
    "    Encodes the 'diabetes' column of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - diabetes_column: pandas.Series, the 'diabetes' column from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - y_encoded: The encoded labels for the 'diabetes' column.\n",
    "    - class_labels: The original string labels corresponding to the encoded labels.\n",
    "    \"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    y_encoded = encoder.fit_transform(diabetes_column)\n",
    "    class_labels = encoder.classes_  # Stores the original string labels\n",
    "    return class_labels\n",
    "\n",
    "\n",
    "class_labels = encode_diabetes_column(dataset[\"diabetes\"])\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_labels,\n",
    "    yticklabels=class_labels,\n",
    ")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the dataset into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = dataset_without_extended_diabetes_categories\n",
    "diabetes_X, diabetes_y = dataset.drop(columns=\"diabetes\"), dataset.diabetes\n",
    "\n",
    "diabetes_train_X, diabetes_test_X, diabetes_train_y, diabetes_test_y = train_test_split(\n",
    "    diabetes_X, diabetes_y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a column transformer to handle categorical data for certain models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Step 2: Create the ColumnTransformer\n",
    "# We use 'remainder='passthrough'' to keep the non-categorical columns unchanged\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"OHE\",\n",
    "            OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"),\n",
    "            make_column_selector(dtype_include=\"category\"),\n",
    "        )\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Step 3: Fit and Transform the Data\n",
    "# The transformed data will be a NumPy array\n",
    "transformed_data = column_transformer.fit_transform(dataset)\n",
    "\n",
    "# Optional: Convert the transformed data back to a DataFrame\n",
    "# This step requires generating the new column names after transformation\n",
    "new_columns = column_transformer.get_feature_names_out()\n",
    "\n",
    "# Creating a new DataFrame with the transformed data and new column names\n",
    "one_hot_encoded_dataset = pd.DataFrame(transformed_data, columns=new_columns) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review our one hot encoded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And next a perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes_X, diabetes_y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "perceptron_model = make_pipeline(\n",
    "    column_transformer,\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    Perceptron()\n",
    ")\n",
    "\n",
    "params_grid = {\n",
    "    \"perceptron__class_weight\": [None, 'balanced'],\n",
    "    \"perceptron__penalty\": [None, 'l2', 'l1', 'elasticnet'],\n",
    "    \"perceptron__alpha\": [0.0001, 0.001, 0.01, 0.1],\n",
    "    \"perceptron__eta0\": [0.1, 1, 10],\n",
    "    \"perceptron__shuffle\": [True, False]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(perceptron_model, params_grid, cv=10, scoring=\"recall_macro\", n_jobs=-1)\n",
    "#clf.fit(X_train[:2000], y_train[:2000])\n",
    "\n",
    "#print(clf.best_params_)\n",
    "#print(classification_report(y_test, clf.predict(X_test), zero_division=0.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on grid search we choose to create this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "perceptron_final_model = make_pipeline(\n",
    "    column_transformer,\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    Perceptron(alpha=0.0001, class_weight=\"balanced\", eta0=0.1, penalty='l1', shuffle=False)\n",
    ")\n",
    "\n",
    "perceptron_final_model.fit(diabetes_train_X[:10000], diabetes_train_y[:10000])\n",
    "\n",
    "svm_predicted = perceptron_final_model.predict(diabetes_test_X)\n",
    "\n",
    "\n",
    "print(f\"Recall:\\t\\t{recall_score(diabetes_test_y, svm_predicted, pos_label='Diabetes.YES')}\")\n",
    "print(f\"Precision:\\t{precision_score(diabetes_test_y, svm_predicted, pos_label='Diabetes.YES')}\")\n",
    "print(f\"F1:\\t\\t{f1_score(diabetes_test_y, svm_predicted, pos_label='Diabetes.YES')}\")\n",
    "print(f\"Accuracy:\\t{accuracy_score(diabetes_test_y, svm_predicted)}\")\n",
    "\n",
    "# Display confusion matrix\n",
    "cm = confusion_matrix(diabetes_test_y, svm_predicted)\n",
    "ConfusionMatrixDisplay(cm, display_labels=perceptron_final_model.classes_).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's look at the SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SVM Model\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "svm_column_transformer = make_column_transformer(\n",
    "    (MinMaxScaler(), make_column_selector(dtype_include=[\"int64\", \"float64\"])),\n",
    "    (OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=\"category\")),\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "\n",
    "svm_pipeline = make_pipeline(\n",
    "    svm_column_transformer,\n",
    "    KNNImputer(n_neighbors=3),\n",
    "    MinMaxScaler(feature_range=(0, 1)),\n",
    "    SVC(kernel=\"poly\", C=8, degree=5, class_weight=\"balanced\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "\n",
    "\n",
    "params = [\n",
    "    {\n",
    "    # \"knnimputer__n_neighbors\": [1, 2, 3, 5],\n",
    "    \"svc__kernel\": [\"poly\"],\n",
    "    \"svc__C\": [0.5, 1, 2, 4, 8, 16],\n",
    "    \"svc__degree\": [1, 2, 3, 4, 5, 6],\n",
    "    },\n",
    "    {\n",
    "    # \"knnimputer__n_neighbors\": [1, 2, 3, 5],\n",
    "    \"svc__kernel\": [\"rbf\"],\n",
    "    \"svc__C\": [0.001, 0.05, 0.5, 1, 2, 4, 8, 16, 20],\n",
    "    \"svc__gamma\": [\"auto\", 0.0001, .0002, 0.00025, 0.0003, 0.0005, 0.001, 0.01, 0.1, 1, 2, 4, 8]\n",
    "    },\n",
    "    {\n",
    "    # \"knnimputer__n_neighbors\": [1, 2, 3, 5],\n",
    "    \"svc__kernel\": [\"linear\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "recall_diabetes = make_scorer(recall_score, pos_label=\"Diabetes.YES\")\n",
    "grid_search = GridSearchCV(svm_pipeline, cv=3, param_grid=params, scoring=recall_diabetes, verbose=2, n_jobs=3)\n",
    "# grid_search.fit(diabetes_train_X, diabetes_train_y)\n",
    "# grid_search.best_score_\n",
    "# grid_search.best_params # kernel='rbf'; C=0.05; gamma=0.1; n_neighbors=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Final SVM pipeline\n",
    "svm_pipeline = make_pipeline(\n",
    "    column_transformer,\n",
    "    KNNImputer(n_neighbors=3),\n",
    "    MinMaxScaler(feature_range=(0, 1)),\n",
    "    SVC(kernel=\"rbf\", C=0.05, gamma=0.1, class_weight=\"balanced\",)\n",
    ")\n",
    "\n",
    "svm_pipeline.fit(diabetes_train_X[:10000], diabetes_train_y[:10000])\n",
    "\n",
    "svm_predicted = svm_pipeline.predict(diabetes_test_X)\n",
    "\n",
    "\n",
    "print(f\"Recall:\\t\\t{recall_score(diabetes_test_y, svm_predicted, pos_label='Diabetes.YES')}\")\n",
    "print(f\"Precision:\\t{precision_score(diabetes_test_y, svm_predicted, pos_label='Diabetes.YES')}\")\n",
    "print(f\"F1:\\t\\t{f1_score(diabetes_test_y, svm_predicted, pos_label='Diabetes.YES')}\")\n",
    "print(f\"Accuracy:\\t{accuracy_score(diabetes_test_y, svm_predicted)}\")\n",
    "\n",
    "# Display confusion matrix\n",
    "cm = confusion_matrix(diabetes_test_y, svm_predicted)\n",
    "ConfusionMatrixDisplay(cm, display_labels=svm_pipeline.classes_).plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

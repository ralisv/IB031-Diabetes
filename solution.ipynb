{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes Classification\n",
    "\n",
    "## About dataset\n",
    "- The Behavioral Risk Factor Surveillance System (BRFSS) is a health-related telephone survey that collects data from U.S. residents on their health-related risk behaviors, chronic health conditions, and use of preventive services\n",
    "- The dataset has been established in 1984 with 15 states, it now collects data from all 50 states, D.C., and 3 U.S. territories\n",
    "- Over 400,000 adult interviews are completed each year, making it the largest continuous health survey system in the world\n",
    "- Factors assessed include tobacco use, healthcare coverage, HIV/AIDS knowledge/prevention, physical activity, and fruit/vegetable consumption\n",
    "- A record in the data corresponds to a single respondent (each from a single household)\n",
    "- The description of columns can be found in the linked PDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features description\n",
    "| Feature               | Description                                                                  |\n",
    "|-----------------------|------------------------------------------------------------------------------|\n",
    "| diabetes              | Subject was told they have diabetes                                          |\n",
    "| high_blood_pressure   | Subject has high blood pressure                                              |\n",
    "| high_cholesterol      | Subject has high cholesterol                                                 |\n",
    "| cholesterol_check     | Subject had cholesterol check within the last five years                     |\n",
    "| bmi                   | BMI of the subject                                                           |\n",
    "| smoked_100_cigarettes | Subject has smoked at least 100 cigarettes during their life                 |\n",
    "| stroke                | Subject experienced stroke during their life                                 |\n",
    "| coronary_disease      | Subject has/had coronary heart disease or myocardial infarction              |\n",
    "| exercise              | Subject does regular exercise or physical activity                           |\n",
    "| consumes_fruit        | Subject consumes fruits at least once a day                                  |\n",
    "| consumes_vegetables   | Subject consumes vegetables at least once a day                              |\n",
    "| heavy_alcohol_drinker | Heavy drinkers are defined as adult men having more than 14 drinks per week |\n",
    "| insurance             | Subject has some kind of health plan (insurance, prepaid plans, ...)         |\n",
    "| no_doctor_money       | Subject was unable to visit doctor in the past 12 months because of cost     |\n",
    "| health                | How good is the health of the subject (self rated)                           |\n",
    "| mental_health         | Number of days in the past month when subject's mental health was not good   |\n",
    "| physical_health       | Number of days in the past month when subject's physical health was not good |\n",
    "| climb_difficulty      | Subject has difficulties climbing stairs                                     |\n",
    "| sex                   | Sex of the subject                                                           |\n",
    "| age_category          | Age category of the subject                                                  |\n",
    "| educatation_level     | Highest level of education achieved by the subject                           |\n",
    "| income                | Income of subject's household                                                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset. All 5 parts are concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from core import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do basic preprocessing on columns and categorical values in order to make the dataset more humanly readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import process_columns, remove_unusable_diabetes_categories\n",
    "\n",
    "process_columns(dataset)\n",
    "\n",
    "# 'Unnamed: 0' is a duplicate column of ID\n",
    "dataset.drop(\"Unnamed: 0\", axis=\"columns\", inplace=True)\n",
    "\n",
    "dataset.drop_duplicates(inplace=True)\n",
    "\n",
    "    # ID is no longer needed after dropping duplicates\n",
    "dataset.drop(\"ID\", axis=\"columns\", inplace=True)\n",
    "\n",
    "# Remove rows where target label is missing\n",
    "dataset = dataset[~dataset[\"diabetes\"].isna()]\n",
    "\n",
    "# Random forest model can actually handle classification into multiple categories, so we create a copy\n",
    "dataset_with_extended_diabetes_categories = dataset.copy()\n",
    "\n",
    "# Remove pre diabetes and diabetes in pregnancy categories\n",
    "dataset_without_extended_diabetes_categories = remove_unusable_diabetes_categories(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a baseline model to have something to compare our more advanced models to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = dataset.drop(\"diabetes\", axis=\"columns\"), dataset[\"diabetes\"]\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=0)\n",
    "\n",
    "baseline = DummyClassifier(strategy=\"uniform\", random_state=0)\n",
    "baseline.fit(train_X, train_y)\n",
    "print(classification_report(test_y, baseline.predict(test_X), zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Assuming dataset_with_extended_diabetes_categories is a pandas DataFrame\n",
    "dataset = dataset_with_extended_diabetes_categories\n",
    "\n",
    "# Splitting dataset into features and target\n",
    "X = dataset.drop(\"diabetes\", axis=1)\n",
    "y = dataset[\"diabetes\"]\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Splitting dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create preprocessing steps\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OrdinalEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "# Create the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_transformer, make_column_selector(dtype_include=\"category\")),\n",
    "        (\"num\", numerical_transformer, make_column_selector(dtype_exclude=\"category\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            RandomForestClassifier(class_weight=\"balanced\", max_features=12, min_samples_leaf=3, max_depth=20, criterion=\"gini\", n_estimators=100, random_state=42),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "random_forest_model = pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our first model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def encode_diabetes_column(diabetes_column):\n",
    "    \"\"\"\n",
    "    Encodes the 'diabetes' column of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - diabetes_column: pandas.Series, the 'diabetes' column from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - y_encoded: The encoded labels for the 'diabetes' column.\n",
    "    - class_labels: The original string labels corresponding to the encoded labels.\n",
    "    \"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    y_encoded = encoder.fit_transform(diabetes_column)\n",
    "    class_labels = encoder.classes_  # Stores the original string labels\n",
    "    return class_labels\n",
    "\n",
    "\n",
    "class_labels = encode_diabetes_column(dataset[\"diabetes\"])\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_labels,\n",
    "    yticklabels=class_labels,\n",
    ")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the dataset into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = dataset_without_extended_diabetes_categories\n",
    "diabetes_X, diabetes_y = dataset.drop(columns=\"diabetes\"), dataset.diabetes\n",
    "\n",
    "diabetes_train_X, diabetes_test_X, diabetes_train_y, diabetes_test_y = train_test_split(\n",
    "    diabetes_X, diabetes_y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a column transformer to handle categorical data for certain models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Step 2: Create the ColumnTransformer\n",
    "# We use 'remainder='passthrough'' to keep the non-categorical columns unchanged\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"OHE\",\n",
    "            OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"),\n",
    "            make_column_selector(dtype_include=\"category\"),\n",
    "        )\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Step 3: Fit and Transform the Data\n",
    "# The transformed data will be a NumPy array\n",
    "transformed_data = column_transformer.fit_transform(dataset.drop(\"diabetes\"))\n",
    "\n",
    "# Optional: Convert the transformed data back to a DataFrame\n",
    "# This step requires generating the new column names after transformation\n",
    "new_columns = column_transformer.get_feature_names_out()\n",
    "\n",
    "# Creating a new DataFrame with the transformed data and new column names\n",
    "one_hot_encoded_dataset = pd.DataFrame(transformed_data, columns=new_columns) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review our one hot encoded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our second model, a KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our last model a perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_encoded_dataset, diabetes_y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "perceptron_model = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    Perceptron()\n",
    ")\n",
    "\n",
    "# Train the perceptron model.\n",
    "perceptron_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_pred = perceptron_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
